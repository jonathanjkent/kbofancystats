# Add to ploys df
ploys <- polys %>% left_join(income, by = "CUSEC")
summary(polys)
summary(ploys)
# Add to ploys df
polys <- polys %>% left_join(income, by = "CUSEC")
library(tidyverse)
library(sf)
# Load Data as SF Objects
data <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
data$fake_lon <- data$masked_lon + runif(nrow(data), 0, .025)
data$fake_lat <- data$masked_lat + runif(nrow(data), 0, .025)
points <- data %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
polys <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
polys <- st_transform(polys, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Point in polygon function
polys$pt_count <- lengths(st_intersects(polys, points))
# Collapse by community or municipality
#communities <- st_make_valid(polys) %>% group_by(NCA) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
#municipalities <- st_make_valid(polys) %>% group_by(NMUN) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
income$Renta_Persona <- as.numeric(income$Renta_Persona)
income$Renta_Hogar <- as.numeric(income$Renta_Hogar)
income <- income %>% separate(Area, c("CUSEC", "NMUN"))
# Add to ploys df
polys <- polys %>% left_join(income, by = "CUSEC")
library(ggplot2)
ggplot(polys,aes(Renta_Hogar,pt_count))
ggplot(polys,aes(Renta_Hogar,pt_count)) + geom_point()
# Twitter data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets
points <- data %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
polys$tw_count <- lengths(st_intersects(polys, tweets))
summary(polys)
ggplot(polys,aes(tw_count,pt_count)) + geom_point()
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(data), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(data), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
districts$tw_count <- lengths(st_intersects(districts, tweets))
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(data), 0, .025)
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(bgtracks), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(bgtracks), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
districts$tw_count <- lengths(st_intersects(districts, tweets))
income <- read_csv("~/research/incomebybarri.csv")
income$renta_persona <- as.numeric(income$renta_persona)
income$renta_hogar <- as.numeric(income$renta_hogar)
income <- income %>% separate(area, c("CUSEC", "NMUN"))
districts <- districts %>% left_join(income, by = "CUSEC")
# Background tracks and tweets as proportion
districts$bt_pct <- districts$bt_count/sum(districts$bt_count)
districts$tw_pct <- districts$tw_count/sum(districts$tw_count)
ggplot(districts,aes(tw_pct,bt_pct)) + geom_point()
districts$more_tweets <- districts$tw_pct - districts$bt_pct
ggplot(districts,aes(more_tweets,renta_hogar)) + geom_point()
ggplot(districts,aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
ggplot(filter(districts, tw_count>0),aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
filter(districts, tw_count>0),
filter(districts, tw_count>0)
filter(districts, tw_count>10)
ggplot(filter(districts, tw_count>10),aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
View(districts)
# Collapse by community or municipality
#communities <- st_make_valid(polys) %>% group_by(NCA) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count)) %>% summarise(tw_count = sum(tw_count))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count))
View(tweets)
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(bgtracks), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(bgtracks), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
View(districts)
districts$tw_count <- lengths(st_intersects(districts, tweets))
# Background tracks and tweets as proportion
districts$bt_pct <- districts$bt_count/sum(districts$bt_count)
districts$tw_pct <- districts$tw_count/sum(districts$tw_count)
districts$more_tweets <- districts$tw_pct - districts$bt_pct
# Load and add income data to district data
income <- read_csv("~/research/incomebybarri.csv")
income$renta_persona <- as.numeric(income$renta_persona)
income$renta_hogar <- as.numeric(income$renta_hogar)
income <- income %>% separate(area, c("CUSEC", "NMUN2"))
districts <- districts %>% left_join(income, by = "CUSEC")
# Collapse by community or municipality
#communities <- st_make_valid(districts) %>% group_by(NCA) %>% summarise(bt_count = sum(bt_count)) %>% arrange(desc(obs))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count)) %>% summarise(tw_count = sum(tw_count))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count))
munitweets <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(tw_count = sum(tw_count))
municipalities <- municipalities %>% left_join(munitweets, by = "NMUN")
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise_at(vars(bt_count, tw_count), sum)
municipalities
municipalities$bt_pct <- municipalities$bt_count/sum(municipalities$bt_count)
municipalities$tw_pct <- municipalities$tw_count/sum(municipalities$tw_count)
municipalities$more_tweets <- municipalities$tw_pct - municipalities$bt_pct
View(municipalities)
communities <- st_make_valid(districts) %>% group_by(NCA) %>% summarise_at(vars(bt_count, tw_count), sum)
communities$bt_pct <- communities$bt_count/sum(communities$bt_count)
communities$tw_pct <- communities$tw_count/sum(communities$tw_count)
communities$more_tweets <- communities$tw_pct - communities$bt_pct
View(communities)
communities
communities %>% arrange(more_tweets)
municipalities %>% arrange(more_tweets)
# CSV exports
write_csv(communities, "~/research/communities.csv")
write_csv(municipalities, "~/research/municipalities.csv")
municipalities %>% arrange(desc(more_tweets)
)
# Names of all required packages that we will be using during the course
all_pkgs <- c('tidyverse', 'jtools', 'sjPlot','ggplot2','ggthemes','haven','foreign','essurvey','stargazer','knitr','prais','orcutt','fastDummies')
# Install all the packages (it may take a few minutes to install all of them)
install.packages(all_pkgs, dependencies = TRUE)
# Verify the installation worked correctly
setdiff(all_pkgs, row.names(installed.packages()))
library(essurvey)
?essurvey
source("~/Google Drive/Futbol/c19update.R")
library(tidyverse)
library(lubridate)
library(ggplot2)
# Read in stats and fix times
swimstats <- read_csv("~/Google Drive/Personal Documents/swimstats.csv")
swimstats$time <- duration(as.numeric(sub("s.*", "", swimstats$time)), "seconds")
swimstats$fast.50 <- duration(as.numeric(sub("s.*", "", swimstats$fast.50)), "seconds")
swimstats$fast.100 <- duration(as.numeric(sub("s.*", "", swimstats$fast.100)), "seconds")
swimstats$fast.200 <- duration(as.numeric(sub("s.*", "", swimstats$fast.200)), "seconds")
# Add new swim
newswim <- tibble(as_date("2020-08-18"), # Date
duration(60*36+17, "seconds"), # Time
1150, # Meters
317, # Calories
duration(58.0, "seconds"), # Fastest 50m Freestyle
duration(60*2+08.3, "seconds"), # Fastest 100m Freestyle
duration(60*4+37.4, "seconds"), # Fastest 200m Freestyle
2.45, # Meters Per Stroke (DPS)
3.18) # Seconds Per Stroke (Stroke Rate)
newswim[1,10] <- newswim[1,8]/newswim[1,9] # Meters Per Second
swimstats[(nrow(swimstats)+1),] <- newswim
View(swimstats)
# Overwrite csv
write_csv(swimstats,"~/Google Drive/Personal Documents/swimstats.csv")
suppressMessages(library(tidyverse))
suppressMessages(library(plotly))
suppressMessages(library(data.table))
options(dplyr.summarise.inform=F)
data <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/xuwf-dxjd/rows.csv", col_types = cols()))
data$date <- as.Date(data$TipusCasData, format = "%d/%m/%Y")
update <- suppressMessages(read_csv("~/Google Drive/Futbol/updateinfo.csv"))
updaterow <- tibble(Sys.time(), nrow(data))
update[(nrow(update)+1),] <- updaterow
write_csv(update, "~/Google Drive/Futbol/updateinfo.csv")
cat <- data %>% filter(TipusCasDescripcio == "Positiu PCR") %>% filter(!RegioSanitariaDescripcio == "No classificat") %>% group_by(date) %>% summarise(cases = sum(NumCasos))
zeros <- data.frame(c(as.Date("2020-02-20"),as.Date("2020-02-21"),as.Date("2020-02-22"),as.Date("2020-02-23"),as.Date("2020-02-24"),as.Date("2020-02-25"),as.Date("2020-02-26"),as.Date("2020-02-27"),as.Date("2020-03-01"),as.Date("2020-03-03")))
zeros$cases <- 0
names(zeros) <- names(cat)
cat <- rbind(zeros, cat)
cat <- cat %>% arrange(date) %>% mutate(casos = frollmean(cases, 7)) %>% arrange(desc(date))
cat
deaths <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/623z-r97q/rows.csv", col_types = cols()))
View(deaths)
View(cat)
View(deaths)
deaths
deaths$Data <- as.Date(deaths$Data, format = "%d/%m/%Y")
deaths
cat
View(deaths)
deaths <- deaths %>% select(Data, `Defuncions diàries`)
deaths
names(deaths) <- c("date", "defunctions")
deaths
cat <- cat %>% left_join(deaths, by = "date")
cat
cat$defunctions[is.na(cat$defunctions)] <- 0
cat
cat <- cat %>% mutate(defunctions = frollmean(defunctions, 7))
cat
suppressMessages(library(tidyverse))
suppressMessages(library(plotly))
suppressMessages(library(data.table))
options(dplyr.summarise.inform=F)
data <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/xuwf-dxjd/rows.csv", col_types = cols()))
data$date <- as.Date(data$TipusCasData, format = "%d/%m/%Y")
update <- suppressMessages(read_csv("~/Google Drive/Futbol/updateinfo.csv"))
updaterow <- tibble(Sys.time(), nrow(data))
update[(nrow(update)+1),] <- updaterow
write_csv(update, "~/Google Drive/Futbol/updateinfo.csv")
cat <- data %>% filter(TipusCasDescripcio == "Positiu PCR") %>% filter(!RegioSanitariaDescripcio == "No classificat") %>% group_by(date) %>% summarise(cases = sum(NumCasos))
zeros <- data.frame(c(as.Date("2020-02-20"),as.Date("2020-02-21"),as.Date("2020-02-22"),as.Date("2020-02-23"),as.Date("2020-02-24"),as.Date("2020-02-25"),as.Date("2020-02-26"),as.Date("2020-02-27"),as.Date("2020-03-01"),as.Date("2020-03-03")))
zeros$cases <- 0
names(zeros) <- names(cat)
cat <- rbind(zeros, cat)
cat <- cat %>% arrange(date) %>% mutate(casos = frollmean(cases, 7)) %>% arrange(desc(date))
deaths <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/623z-r97q/rows.csv", col_types = cols()))
deaths$Data <- as.Date(deaths$Data, format = "%d/%m/%Y")
deaths <- deaths %>% select(Data, `Defuncions diàries`)
names(deaths) <- c("date", "defunctions")
cat <- cat %>% left_join(deaths, by = "date")
cat$defunctions[is.na(cat$defunctions)] <- 0
cat <- cat %>% mutate(defunctions = frollmean(defunctions, 7)) %>% filter(date>as.Date("2020-02-25"))
cat
names(cat)[1]
names(cat)[1] <- "data"
p0 <- ggplot(cat %>% filter(data != max(cat$data))) + geom_line(aes(data, casos), color = "red") + theme_minimal() + geom_vline(aes(xintercept=as.numeric(as.Date("2020-05-04"))), linetype='dashed', color='lightgrey') + geom_vline(aes(xintercept=as.numeric(as.Date("2020-06-19"))), linetype='dashed', color='lightgrey') + labs(x = "", y = "Mitjana de positius PCR a 7 dies")
p0 <- ggplotly(p0, width=875, height=600)
p0 <- config(p0, displayModeBar = FALSE)
p0
suppressMessages(library(tidyverse))
suppressMessages(library(plotly))
suppressMessages(library(data.table))
options(dplyr.summarise.inform=F)
data <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/xuwf-dxjd/rows.csv", col_types = cols()))
data$date <- as.Date(data$TipusCasData, format = "%d/%m/%Y")
update <- suppressMessages(read_csv("~/Google Drive/Futbol/updateinfo.csv"))
updaterow <- tibble(Sys.time(), nrow(data))
update[(nrow(update)+1),] <- updaterow
write_csv(update, "~/Google Drive/Futbol/updateinfo.csv")
cat <- data %>% filter(TipusCasDescripcio == "Positiu PCR") %>% filter(!RegioSanitariaDescripcio == "No classificat") %>% group_by(date) %>% summarise(cases = sum(NumCasos))
zeros <- data.frame(c(as.Date("2020-02-20"),as.Date("2020-02-21"),as.Date("2020-02-22"),as.Date("2020-02-23"),as.Date("2020-02-24"),as.Date("2020-02-25"),as.Date("2020-02-26"),as.Date("2020-02-27"),as.Date("2020-03-01"),as.Date("2020-03-03")))
zeros$cases <- 0
names(zeros) <- names(cat)
cat <- rbind(zeros, cat)
cat <- cat %>% arrange(date) %>% mutate(casos = frollmean(cases, 7)) %>% arrange(desc(date))
deaths <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/623z-r97q/rows.csv", col_types = cols()))
deaths$Data <- as.Date(deaths$Data, format = "%d/%m/%Y")
deaths <- deaths %>% select(Data, `Defuncions diàries`)
names(deaths) <- c("date", "defunctions")
cat <- cat %>% left_join(deaths, by = "date")
cat$defunctions[is.na(cat$defunctions)] <- 0
cat <- cat %>% mutate(defunctions = frollmean(defunctions, 7)) %>% filter(date>as.Date("2020-02-25")) %>% select(-cases)
names(cat)[1] <- "data"
cat
cat %>% pivot_longer(c(`casos`, `defunctions`), names_to = "tipo", values_to = "mitjana")
cat <- cat %>% pivot_longer(c(`casos`, `defunctions`), names_to = "tipo", values_to = "mitjana")
cat
suppressMessages(library(tidyverse))
suppressMessages(library(plotly))
suppressMessages(library(data.table))
options(dplyr.summarise.inform=F)
data <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/xuwf-dxjd/rows.csv", col_types = cols()))
data$date <- as.Date(data$TipusCasData, format = "%d/%m/%Y")
update <- suppressMessages(read_csv("~/Google Drive/Futbol/updateinfo.csv"))
updaterow <- tibble(Sys.time(), nrow(data))
update[(nrow(update)+1),] <- updaterow
write_csv(update, "~/Google Drive/Futbol/updateinfo.csv")
cat <- data %>% filter(TipusCasDescripcio == "Positiu PCR") %>% filter(!RegioSanitariaDescripcio == "No classificat") %>% group_by(date) %>% summarise(cases = sum(NumCasos))
zeros <- data.frame(c(as.Date("2020-02-20"),as.Date("2020-02-21"),as.Date("2020-02-22"),as.Date("2020-02-23"),as.Date("2020-02-24"),as.Date("2020-02-25"),as.Date("2020-02-26"),as.Date("2020-02-27"),as.Date("2020-03-01"),as.Date("2020-03-03")))
zeros$cases <- 0
names(zeros) <- names(cat)
cat <- rbind(zeros, cat)
cat <- cat %>% arrange(date) %>% mutate(casos = frollmean(cases, 7)) %>% arrange(desc(date))
deaths <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/623z-r97q/rows.csv", col_types = cols()))
deaths$Data <- as.Date(deaths$Data, format = "%d/%m/%Y")
deaths <- deaths %>% select(Data, `Defuncions diàries`)
names(deaths) <- c("date", "defuncions")
cat <- cat %>% left_join(deaths, by = "date")
cat$defuncions[is.na(cat$defuncions)] <- 0
cat <- cat %>% arrange(date) %>% mutate(defuncions = frollmean(defuncions, 7)) %>% filter(date>as.Date("2020-02-25")) %>% select(-cases) %>% arrange(desc(date))
names(cat)[1] <- "data"
cat
cat <- cat %>% pivot_longer(c(`casos`, `defuncions`), names_to = "tipo", values_to = "mitjana")
cat
cat
p0 <- ggplot(cat %>% filter(data != max(cat$data))) + geom_line(aes(data, mitjana, color = tipo)) + theme_minimal() + geom_vline(aes(xintercept=as.numeric(as.Date("2020-05-04"))), linetype='dashed', color='lightgrey') + geom_vline(aes(xintercept=as.numeric(as.Date("2020-06-19"))), linetype='dashed', color='lightgrey') + labs(x = "", y = "Mitjana de positius PCR a 7 dies")
p0 <- ggplotly(p0, width=875, height=600)
p0 <- config(p0, displayModeBar = FALSE)
p0
names(cat) <- c("data", "PCR Positius", "Defuncions")
suppressMessages(library(tidyverse))
suppressMessages(library(plotly))
suppressMessages(library(data.table))
options(dplyr.summarise.inform=F)
data <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/xuwf-dxjd/rows.csv", col_types = cols()))
data$date <- as.Date(data$TipusCasData, format = "%d/%m/%Y")
update <- suppressMessages(read_csv("~/Google Drive/Futbol/updateinfo.csv"))
updaterow <- tibble(Sys.time(), nrow(data))
update[(nrow(update)+1),] <- updaterow
write_csv(update, "~/Google Drive/Futbol/updateinfo.csv")
cat <- data %>% filter(TipusCasDescripcio == "Positiu PCR") %>% filter(!RegioSanitariaDescripcio == "No classificat") %>% group_by(date) %>% summarise(cases = sum(NumCasos))
zeros <- data.frame(c(as.Date("2020-02-20"),as.Date("2020-02-21"),as.Date("2020-02-22"),as.Date("2020-02-23"),as.Date("2020-02-24"),as.Date("2020-02-25"),as.Date("2020-02-26"),as.Date("2020-02-27"),as.Date("2020-03-01"),as.Date("2020-03-03")))
zeros$cases <- 0
names(zeros) <- names(cat)
cat <- rbind(zeros, cat)
cat <- cat %>% arrange(date) %>% mutate(casos = frollmean(cases, 7)) %>% arrange(desc(date))
deaths <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/623z-r97q/rows.csv", col_types = cols()))
deaths$Data <- as.Date(deaths$Data, format = "%d/%m/%Y")
deaths <- deaths %>% select(Data, `Defuncions diàries`)
names(deaths) <- c("date", "defuncions")
cat <- cat %>% left_join(deaths, by = "date")
cat$defuncions[is.na(cat$defuncions)] <- 0
cat <- cat %>% arrange(date) %>% mutate(defuncions = frollmean(defuncions, 7)) %>% filter(date>as.Date("2020-02-25")) %>% select(-cases) %>% arrange(desc(date))
names(cat) <- c("data", "PCR Positius", "Defuncions")
cat
cat <- cat %>% pivot_longer(c(`PCR Positius`, `Defuncions`), names_to = "tipo", values_to = "mitjana")
p0 <- ggplot(cat %>% filter(data != max(cat$data))) + geom_line(aes(data, mitjana, color = tipo)) + theme_minimal() + geom_vline(aes(xintercept=as.numeric(as.Date("2020-05-04"))), linetype='dashed', color='lightgrey') + geom_vline(aes(xintercept=as.numeric(as.Date("2020-06-19"))), linetype='dashed', color='lightgrey') + labs(x = "", y = "Mitjana a 7 dies")
p0 <- ggplotly(p0, width=875, height=600)
p0 <- config(p0, displayModeBar = FALSE)
p0
p0 <- ggplot(cat %>% filter(data != max(cat$data))) + geom_line(aes(data, mitjana, color = tipo)) + theme_minimal() + geom_vline(aes(xintercept=as.numeric(as.Date("2020-05-04"))), linetype='dashed', color='lightgrey') + geom_vline(aes(xintercept=as.numeric(as.Date("2020-06-19"))), linetype='dashed', color='lightgrey') + labs(x = "", y = "Mitjana a 7 dies", color ="")
p0 <- ggplotly(p0, width=875, height=600)
p0 <- ggplotly(p0, width=875, height=600)
p0 <- config(p0, displayModeBar = FALSE)
p0
p0 <- ggplot(cat %>% filter(data != max(cat$data))) + geom_line(aes(data, mitjana, color = tipo)) + theme_minimal() + geom_vline(aes(xintercept=as.numeric(as.Date("2020-05-04"))), linetype='dashed', color='lightgrey') + geom_vline(aes(xintercept=as.numeric(as.Date("2020-06-19"))), linetype='dashed', color='lightgrey') + labs(x = "", y = "Mitjana a 7 dies", color ="")
p0 <- ggplotly(p0, width=875, height=600)
p0
p <- ggplot(cat %>% filter(data != max(cat$data))) + geom_line(aes(data, mitjana, color = tipo)) + theme_minimal() + geom_vline(aes(xintercept=as.numeric(as.Date("2020-05-04"))), linetype='dashed', color='lightgrey') + geom_vline(aes(xintercept=as.numeric(as.Date("2020-06-19"))), linetype='dashed', color='lightgrey') + labs(x = "", y = "Mitjana a 7 dies", color ="")
p <- ggplotly(p0, width=875, height=600)
p
library(tidyverse)
library(lubridate)
library(ggplot2)
# Read in stats and fix times
swimstats <- read_csv("~/Google Drive/Personal Documents/swimstats.csv")
swimstats$time <- duration(as.numeric(sub("s.*", "", swimstats$time)), "seconds")
swimstats$fast.50 <- duration(as.numeric(sub("s.*", "", swimstats$fast.50)), "seconds")
swimstats$fast.100 <- duration(as.numeric(sub("s.*", "", swimstats$fast.100)), "seconds")
swimstats$fast.200 <- duration(as.numeric(sub("s.*", "", swimstats$fast.200)), "seconds")
ggplot(swimstats) + geom_line(aes(meters,calories))
summary(lm(calories ~ meters))
summary(lm(calories ~ meters, data = swimstats))
ggplot(swimstats) + geom_smooth(aes(meters,calories))
summary(lm(calories ~ meters + time, data = swimstats))
ggplot(swimstats) + geom_smooth(aes(time,calories))
ggplot(swimstats) + geom_smooth(aes(date,meters))
ggplot((swimstats %>% filter(date>as.Date("2020-03-01")))) + geom_smooth(aes(date,meters))
ggplot(swimstats) + geom_smooth(aes(date,fast.100))
ggplot((swimstats %>% filter(date>as.Date("2020-03-01")))) + geom_smooth(aes(date,fast.100))
# Plots
ggplot(swimstats %>% filter(date>as.Date("2020-03-01"))) + geom_point(aes(date,fast.100))
# KBO Fancy Stats
source("~/Google Drive/KBO/2020elo.R")
library(tidyverse)
library(lubridate)
library(ggplot2)
# Read in stats and fix times
swimstats <- read_csv("~/Google Drive/Personal Documents/swimstats.csv")
swimstats$time <- duration(as.numeric(sub("s.*", "", swimstats$time)), "seconds")
swimstats$fast.50 <- duration(as.numeric(sub("s.*", "", swimstats$fast.50)), "seconds")
swimstats$fast.100 <- duration(as.numeric(sub("s.*", "", swimstats$fast.100)), "seconds")
swimstats$fast.200 <- duration(as.numeric(sub("s.*", "", swimstats$fast.200)), "seconds")
library(tidyverse)
library(lubridate)
library(ggplot2)
# Read in stats and fix times
swimstats <- read_csv("~/Google Drive/Personal Documents/swimstats.csv")
swimstats$time <- duration(as.numeric(sub("s.*", "", swimstats$time)), "seconds")
swimstats$fast.50 <- duration(as.numeric(sub("s.*", "", swimstats$fast.50)), "seconds")
swimstats$fast.100 <- duration(as.numeric(sub("s.*", "", swimstats$fast.100)), "seconds")
swimstats$fast.200 <- duration(as.numeric(sub("s.*", "", swimstats$fast.200)), "seconds")
# Add new swim
newswim <- tibble(as_date("2020-08-19"), # Date
duration(60*37+47, "seconds"), # Time
1250, # Meters
333, # Calories
NA, # Fastest 50m Freestyle
duration(60*1+58, "seconds"), # Fastest 100m Freestyle
duration(60*4+46.9, "seconds"), # Fastest 200m Freestyle
2.39, # Meters Per Stroke (DPS)
3.14) # Seconds Per Stroke (Stroke Rate)
newswim[1,10] <- newswim[1,8]/newswim[1,9] # Meters Per Second
View(swimstats)
swimstats[(nrow(swimstats)+1),] <- newswim
# Overwrite csv
write_csv(swimstats,"~/Google Drive/Personal Documents/swimstats.csv")
swimstats <- read_csv("~/Google Drive/Personal Documents/swimstats.csv")
swimstats$time <- duration(as.numeric(sub("s.*", "", swimstats$time)), "seconds")
swimstats$fast.50 <- duration(as.numeric(sub("s.*", "", swimstats$fast.50)), "seconds")
swimstats$fast.100 <- duration(as.numeric(sub("s.*", "", swimstats$fast.100)), "seconds")
swimstats$fast.200 <- duration(as.numeric(sub("s.*", "", swimstats$fast.200)), "seconds")
ggplot(swimstats) + geom_smooth(aes(date,long))
swimstats <- read_csv("~/Google Drive/Personal Documents/swimstats.csv")
swimstats$time <- duration(as.numeric(sub("s.*", "", swimstats$time)), "seconds")
swimstats$fast.50 <- duration(as.numeric(sub("s.*", "", swimstats$fast.50)), "seconds")
swimstats$fast.100 <- duration(as.numeric(sub("s.*", "", swimstats$fast.100)), "seconds")
swimstats$fast.200 <- duration(as.numeric(sub("s.*", "", swimstats$fast.200)), "seconds")
ggplot(swimstats) + geom_smooth(aes(date,long))
ggplot(swimstats) + geom_point(aes(date,long))
source("~/Google Drive/Futbol/c19update.R")
source("~/Google Drive/KBO/2020stats.R")
source("~/Google Drive/Futbol/c19update.R")
deaths <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/uqk7-bf9s/rows.csv", col_types = cols()))
deaths
deaths <- deaths %>% select(`Data defunció`, `Nombre defuncions`)
ames(deaths) <- c("date", "defuncions")
names(deaths) <- c("date", "defuncions")
deaths
deaths$date <- as.Date(deaths$date, format = "%d/%m/%Y")
deaths
deaths <- deaths %>% group_by(date) %>% summarise(defuncions = sum(defuncions))
View(deaths)
suppressMessages(library(tidyverse))
suppressMessages(library(plotly))
suppressMessages(library(data.table))
options(dplyr.summarise.inform=F)
data <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/xuwf-dxjd/rows.csv", col_types = cols()))
data$date <- as.Date(data$TipusCasData, format = "%d/%m/%Y")
update <- suppressMessages(read_csv("~/Google Drive/Futbol/updateinfo.csv"))
updaterow <- tibble(Sys.time(), nrow(data))
update[(nrow(update)+1),] <- updaterow
write_csv(update, "~/Google Drive/Futbol/updateinfo.csv")
cat <- data %>% filter(TipusCasDescripcio == "Positiu PCR") %>% filter(!RegioSanitariaDescripcio == "No classificat") %>% group_by(date) %>% summarise(cases = sum(NumCasos))
zeros <- data.frame(c(as.Date("2020-02-20"),as.Date("2020-02-21"),as.Date("2020-02-22"),as.Date("2020-02-23"),as.Date("2020-02-24"),as.Date("2020-02-25"),as.Date("2020-02-26"),as.Date("2020-02-27"),as.Date("2020-03-01"),as.Date("2020-03-03")))
zeros$cases <- 0
names(zeros) <- names(cat)
cat <- rbind(zeros, cat)
cat <- cat %>% arrange(date) %>% mutate(casos = frollmean(cases, 7)) %>% arrange(desc(date))
deaths <- suppressMessages(read_csv("https://analisi.transparenciacatalunya.cat/api/views/uqk7-bf9s/rows.csv", col_types = cols()))
deaths <- deaths %>% select(`Data defunció`, `Nombre defuncions`)
names(deaths) <- c("date", "defuncions")
deaths$date <- as.Date(deaths$date, format = "%d/%m/%Y")
deaths <- deaths %>% group_by(date) %>% summarise(defuncions = sum(defuncions))
cat <- cat %>% left_join(deaths, by = "date")
cat
cat$defuncions[is.na(cat$defuncions)] <- 0
cat <- cat %>% arrange(date) %>% mutate(defuncions = frollmean(defuncions, 7)) %>% filter(date>as.Date("2020-02-25")) %>% select(-cases) %>% arrange(desc(date))
names(cat) <- c("data", "PCR Positius", "Defuncions")
cat <- cat %>% pivot_longer(c(`PCR Positius`, `Defuncions`), names_to = "tipo", values_to = "mitjana")
p0 <- ggplot(cat %>% filter(data != max(cat$data))) + geom_line(aes(data, mitjana, color = tipo)) + theme_minimal() + geom_vline(aes(xintercept=as.numeric(as.Date("2020-05-04"))), linetype='dashed', color='lightgrey') + geom_vline(aes(xintercept=as.numeric(as.Date("2020-06-19"))), linetype='dashed', color='lightgrey') + labs(x = "", y = "Mitjana a 7 dies", color ="")
p0 <- ggplotly(p0, width=875, height=600)
p0 <- config(p0, displayModeBar = FALSE)
p0
library(tidyverse)
library(lubridate)
library(ggplot2)
# Read in stats and fix times
swimstats <- read_csv("~/Google Drive/Personal Documents/swimstats.csv")
swimstats$time <- duration(as.numeric(sub("s.*", "", swimstats$time)), "seconds")
swimstats$fast.50 <- duration(as.numeric(sub("s.*", "", swimstats$fast.50)), "seconds")
swimstats$fast.100 <- duration(as.numeric(sub("s.*", "", swimstats$fast.100)), "seconds")
swimstats$fast.200 <- duration(as.numeric(sub("s.*", "", swimstats$fast.200)), "seconds")
# Add new swim
newswim <- tibble(as_date("2020-08-20"), # Date
duration(60*43+54, "seconds"), # Time
1350, # Meters
376, # Calories
duration(55.5, "seconds"), # Fastest 50m Freestyle
duration(60*1+52, "seconds"), # Fastest 100m Freestyle
duration(60*5+35.6, "seconds"), # Fastest 200m Freestyle
500, # Longest Continuous Distance
2.39, # Meters Per Stroke (DPS)
3.24) # Seconds Per Stroke (Stroke Rate)
newswim[1,11] <- newswim[1,9]/newswim[1,10] # Meters Per Second
View(newswim)
swimstats[(nrow(swimstats)+1),] <- newswim
ggplot(swimstats) + geom_point(aes(meters,meters.per.second))
ggplot(swimstats) + geom_point(aes(meters,meters.per.sec))
# Plots
ggplot(swimstats %>% filter(date>as.Date("2020-03-01"))) + geom_point(aes(meters,meters.per.sec))
# Plots
ggplot(swimstats %>% filter(date>as.Date("2020-03-01"))) + geom_point(aes(long,meters.per.sec))
View(swimstats)
# Overwrite csv
write_csv(swimstats,"~/Google Drive/Personal Documents/swimstats.csv")
# Sums and averages
cat(" ","Swim Stats Summary:"," ",paste(round(sum(swimstats$time)/60/60, digits = 1), "hours, total"),
paste(sum(swimstats$meters), "meters, total"),
paste(round(sum(swimstats$meters)/nrow(swimstats), digits = 1), "meters, per swim")," ", sep="\n")
# KBO Fancy Stats
source("~/Google Drive/KBO/2020elo.R")
source("~/Google Drive/KBO/2020stats.R")
source("~/Google Drive/Futbol/c19update.R")
# KBO Fancy Stats
source("~/Google Drive/KBO/2020elo.R")
