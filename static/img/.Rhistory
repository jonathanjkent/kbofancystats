iran <- rbind(iran, future)
# Make an graph (to easily save as an image)
ggplot(iran,aes(date, cases, color = type)) + geom_line()
# Make an interactive one
plot <- ggplot(iran,aes(date, cases, color = type)) + geom_line()
ggplotly(plot)
View(future)
1+2
1+2 =4
library(tidyverse)
data <- read_csv("~research/data/exp_raw/tigaserver_app_fix.csv")
data <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
View(data)
summary(data)
library(ggmap)
register_google(key = "AIzaSyC8Mmv45YuMzDqM5_BML_35-j5HaSeTyZ4")
spain <-  get_googlemap(center = "Spain", zoom = 6)
spain <-  get_googlemap("Spain", zoom = 6)
spain <-  get_map("Spain", zoom = 6)
?get_googlemap
spain <-  get_map("Madric, Spain", zoom = 6)
g
get_googlemap("waco, texas", maptype = "satellite") %>% ggmap()
register_google(key = "AIzaSyC8Mmv45YuMzDqM5_BML_35-j5HaSeTyZ4")
register_google(key = "AIzaSyAwysDpKGsfTWgDMMDh071KiC-cYPWrXEY")
spain <-  get_map("Madric, Spain", zoom = 6)
ggmap(spain)
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = data)
library(ggplot2)
bcn <-  get_map("Barcelona, Spain", zoom = 12)
ggmap(bcn) + geom_point(aes(x = masked_lon, y = masked_lat), data = data)
data$grid.sq <- paste0(data$masked_lon,"x",data$masked_lat)
by.sq <- data %>% group_by(grid.sq) %>% count()
View(by.sq)
by.sq <- data %>% group_by(grid.sq) %>% count() %>% arrange(n)
View(by.sq)
by.sq <- data %>% group_by(grid.sq) %>% count() %>% arrange(desc(n)
by.sq[1,2]
by.sq[2,1]
by.sq[5,1]
topten <- by.sq[1:10,1]
toptenlocs <- data %>% filter(grid.sq %in% topten)
toptenlocs <- data %>% filter(grid.sq %in% as.list(topten))
topten <- as.list(by.sq[1:10,1])
topten <- as.vector(by.sq[1:10,1])
topten <- as.character(by.sq[1:10,1])
toptenlocs <- data %>% filter(grid.sq %in% topten)
topten
topten <- as.vector(by.sq[1:10,1])
topten
toptenlocs <- data %>% filter(grid.sq %in% topten$grid.sq)
View(toptenlocs)
toptenlocs <- data %>% filter(grid.sq %in% c("-0.025x39.925", "-0.025x39.95 "))
by.sq <- data %>% group_by(grid.sq) %>% count() %>% arrange(desc(n))
by.sq
topten <- by.sq[1:10,1]
toptenlocs <- data %>% filter(grid.sq %in% topten$grid.sq)
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = toptenlocs)
spain <-  get_map("Madrid, Spain", zoom = 4)
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = toptenlocs)
View(topten)
spain <-  get_map("Madrid, Spain", zoom = 6)
topten <- by.sq[1:100,1]
toptenlocs <- data %>% filter(grid.sq %in% topten$grid.sq)
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = toptenlocs)
spain <-  get_map("Zaragoza, Spain", zoom = 6)
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = toptenlocs)
spain <-  get_map("Valencia, Spain", zoom = 6)
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = toptenlocs)
View(by.sq)
test.sq <- data %>% filter(grid.sq == "2.05x41.55")
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
ggmap(bcn) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
bcn <-  get_map("Barcelona, Spain", zoom = 12)
ggmap(bcn) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
bcn <-  get_map("Barcelona, Spain", zoom = 10)
ggmap(bcn) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
sabadell <-  get_map("Sabadell, Spain", zoom = 13)
ggmap(sabadell) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
sabadell <-  get_map("Sabadell, Spain", zoom = 12)
ggmap(sabadell) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
ggmap(sabadell) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
sabadell <-  get_map("Torrebonica, Spain", zoom = 13)
ggmap(sabadell) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
View(test.sq)
test.sq$fake_lon <- test.sq$masked_lon - runif(1, 0, .025)
test.sq$fake_lat <- test.sq$masked_lat - runif(1, 0, .025)
ggmap(sabadell) + geom_point(aes(x = fake_lon, y = fake_lat), data = test.sq)
test.sq$fake_lat <- test.sq$masked_lat + runif(1, 0, .025)
ggmap(sabadell) + geom_point(aes(x = fake_lon, y = fake_lat), data = test.sq)
test.sq$fake_lon <- test.sq$masked_lon - runif(nrow(test.sq), 0, .025)
test.sq$fake_lat <- test.sq$masked_lat + runif(nrow(test.sq), 0, .025)
ggmap(sabadell) + geom_point(aes(x = fake_lon, y = fake_lat), data = test.sq)
ggmap(sabadell) + geom_point(aes(x = fake_lon, y = fake_lat), data = test.sq,alpha = 0.05)
ggmap(sabadell) + geom_point(aes(x = fake_lon, y = fake_lat), data = test.sq, alpha = 0.2)
ggmap(sabadell) + geom_point(aes(x = fake_lon, y = fake_lat), data = test.sq, alpha = 0.1)
data$fake_lon <- data$masked_lon - runif(nrow(data), 0, .025)
data$fake_lat <- data$masked_lat + runif(nrow(data), 0, .025)
ggmap(bcn) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.1)
bcn <-  get_map("Barcelona, Spain", zoom = 15)
ggmap(bcn) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.1)
ggmap(bcn) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.2)
bcn <-  get_map("Barcelona, Spain", zoom = 14)
ggmap(bcn) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.4)
bcn <-  get_map("Barcelona, Spain", zoom = 13)
ggmap(bcn) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.4)
ggmap(bcn) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.2)
madrid <-  get_map("Madrid, Spain", zoom = 13)
ggmap(madrid) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.2)
madrid <-  get_map("Madrid, Spain", zoom = 11)
ggmap(madrid) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.2)
ggmap(madrid) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.2)
View(data)
library(tidyverse)
library(rvest)
d <- read_html("https://www.teenvogue.com/news-politics") %>% html_nodes(.byline-contributor-link)
d <- read_html("https://www.teenvogue.com/news-politics") %>% html_nodes(".byline-contributor-link")
d
library(tidyverse)
library(sf)
# Load Data as SF Objects
data <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
data$fake_lon <- data$masked_lon + runif(nrow(data), 0, .025)
data$fake_lat <- data$masked_lat + runif(nrow(data), 0, .025)
points <- data %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
polys <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
polys <- st_transform(polys, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Point in polygon function
polys$pt_count <- lengths(st_intersects(polys, points))
View(polys)
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
View(income)
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
View(income)
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
income$Renta_Hogar <- as.numeric(income$Renta_Hogar)
income$Renta_Persona <- as.numeric(income$Renta_Persona)
View(polys)
?sep
income <- income %>% separate(Area, c("CUSEC", "NMUN"))
income
# Add to ploys df
ploys <- polys %>% left_join(income, by = "CUSEC")
summary(polys)
summary(ploys)
# Add to ploys df
polys <- polys %>% left_join(income, by = "CUSEC")
library(tidyverse)
library(sf)
# Load Data as SF Objects
data <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
data$fake_lon <- data$masked_lon + runif(nrow(data), 0, .025)
data$fake_lat <- data$masked_lat + runif(nrow(data), 0, .025)
points <- data %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
polys <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
polys <- st_transform(polys, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Point in polygon function
polys$pt_count <- lengths(st_intersects(polys, points))
# Collapse by community or municipality
#communities <- st_make_valid(polys) %>% group_by(NCA) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
#municipalities <- st_make_valid(polys) %>% group_by(NMUN) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
income$Renta_Persona <- as.numeric(income$Renta_Persona)
income$Renta_Hogar <- as.numeric(income$Renta_Hogar)
income <- income %>% separate(Area, c("CUSEC", "NMUN"))
# Add to ploys df
polys <- polys %>% left_join(income, by = "CUSEC")
library(ggplot2)
ggplot(polys,aes(Renta_Hogar,pt_count))
ggplot(polys,aes(Renta_Hogar,pt_count)) + geom_point()
# Twitter data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets
points <- data %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
polys$tw_count <- lengths(st_intersects(polys, tweets))
summary(polys)
ggplot(polys,aes(tw_count,pt_count)) + geom_point()
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(data), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(data), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
districts$tw_count <- lengths(st_intersects(districts, tweets))
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(data), 0, .025)
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(bgtracks), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(bgtracks), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
districts$tw_count <- lengths(st_intersects(districts, tweets))
income <- read_csv("~/research/incomebybarri.csv")
income$renta_persona <- as.numeric(income$renta_persona)
income$renta_hogar <- as.numeric(income$renta_hogar)
income <- income %>% separate(area, c("CUSEC", "NMUN"))
districts <- districts %>% left_join(income, by = "CUSEC")
# Background tracks and tweets as proportion
districts$bt_pct <- districts$bt_count/sum(districts$bt_count)
districts$tw_pct <- districts$tw_count/sum(districts$tw_count)
ggplot(districts,aes(tw_pct,bt_pct)) + geom_point()
districts$more_tweets <- districts$tw_pct - districts$bt_pct
ggplot(districts,aes(more_tweets,renta_hogar)) + geom_point()
ggplot(districts,aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
ggplot(filter(districts, tw_count>0),aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
filter(districts, tw_count>0),
filter(districts, tw_count>0)
filter(districts, tw_count>10)
ggplot(filter(districts, tw_count>10),aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
View(districts)
# Collapse by community or municipality
#communities <- st_make_valid(polys) %>% group_by(NCA) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count)) %>% summarise(tw_count = sum(tw_count))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count))
View(tweets)
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(bgtracks), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(bgtracks), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
View(districts)
districts$tw_count <- lengths(st_intersects(districts, tweets))
# Background tracks and tweets as proportion
districts$bt_pct <- districts$bt_count/sum(districts$bt_count)
districts$tw_pct <- districts$tw_count/sum(districts$tw_count)
districts$more_tweets <- districts$tw_pct - districts$bt_pct
# Load and add income data to district data
income <- read_csv("~/research/incomebybarri.csv")
income$renta_persona <- as.numeric(income$renta_persona)
income$renta_hogar <- as.numeric(income$renta_hogar)
income <- income %>% separate(area, c("CUSEC", "NMUN2"))
districts <- districts %>% left_join(income, by = "CUSEC")
# Collapse by community or municipality
#communities <- st_make_valid(districts) %>% group_by(NCA) %>% summarise(bt_count = sum(bt_count)) %>% arrange(desc(obs))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count)) %>% summarise(tw_count = sum(tw_count))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count))
munitweets <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(tw_count = sum(tw_count))
municipalities <- municipalities %>% left_join(munitweets, by = "NMUN")
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise_at(vars(bt_count, tw_count), sum)
municipalities
municipalities$bt_pct <- municipalities$bt_count/sum(municipalities$bt_count)
municipalities$tw_pct <- municipalities$tw_count/sum(municipalities$tw_count)
municipalities$more_tweets <- municipalities$tw_pct - municipalities$bt_pct
View(municipalities)
communities <- st_make_valid(districts) %>% group_by(NCA) %>% summarise_at(vars(bt_count, tw_count), sum)
communities$bt_pct <- communities$bt_count/sum(communities$bt_count)
communities$tw_pct <- communities$tw_count/sum(communities$tw_count)
communities$more_tweets <- communities$tw_pct - communities$bt_pct
View(communities)
communities
communities %>% arrange(more_tweets)
municipalities %>% arrange(more_tweets)
# CSV exports
write_csv(communities, "~/research/communities.csv")
write_csv(municipalities, "~/research/municipalities.csv")
municipalities %>% arrange(desc(more_tweets)
)
library(blogdown)
library(rmarkdown)
library(tidyverse)
library(rvest)
library(elo)
library(scales)
setwd("~/Google Drive/KBO")
## Scrape 2020 Schedule and Results
links <- read_csv("mykboweeks.csv")
links <- links$url
all.results <- NA
all.schedule <- NA
for (i in links){
print(i)
url <- i
week <- substring(url, first = 38)
d <- read_html(url) %>% html_table() %>% as.data.frame() %>% select(-X2, -X4)
d <- d[2:(nrow(d)-1),]
d <- d %>% filter(str_detect(X1, "2020") == F)
d <- d %>% separate(X1, c("away1", "away2"))
d <- d %>% separate(X5, c("home1", "home2"))
d <- d %>% separate(X3, c("A.Score", "H.Score"))
d$Away <- paste(d$away1, d$away2)
d$Home <- paste(d$home1, d$home2)
d$week <- as.Date(week)
d$Year <- 2020
d$addday <- rep(0:6, each = 5, length.out = nrow(d))
d$Date <- as.Date(d$week + d$addday)
results <- d %>% filter(!str_detect(H.Score, "0am") & !str_detect(A.Score, "Canceled")) %>% select(Date, Home, Away, H.Score, A.Score, Year)
schedule <- d %>% filter(str_detect(H.Score, "0am")) %>% select(Home, Away, Date, Year)
all.results <- rbind(all.results, results)
if (nrow(schedule) > 0) {
all.schedule <- rbind(all.schedule, schedule)
}
}
all.results <- all.results[2:nrow(all.results),]
all.schedule <- all.schedule[2:nrow(all.schedule),]
## Tally Current Win Totals
all.results$H.Score <- as.numeric(all.results$H.Score)
all.results$A.Score <- as.numeric(all.results$A.Score)
all.results <- mutate(all.results, H.Win = ifelse(H.Score > A.Score, 1,
ifelse(H.Score == A.Score, .5, 0)))
all.results <- mutate(all.results, A.Win = ifelse(H.Score < A.Score, 1,
ifelse(H.Score == A.Score, .5, 0)))
h.wins <- all.results %>% select(Home, H.Win)
names(h.wins) <- c("Winner", "n")
a.wins <- all.results %>% select(Away, A.Win)
names(a.wins) <- c("Winner", "n")
wins <- rbind(h.wins,a.wins)
wins <- wins %>% group_by(Winner) %>% summarize(n = sum(n))
all.results <- all.results %>% select(-H.Win, -A.Win)
## Read In Historical Results
kbohistory <- read_csv("kboresults.csv")
kbohistory$Date <- ISOdate(kbohistory$Year, kbohistory$Month, kbohistory$Day)
kbohistory <- kbohistory %>% select(Date, Home, Away, H.Score, A.Score, Year)
all.history <- rbind(kbohistory, all.results)
all.history <- all.history[order(all.history$Date),]
all.history$H.Score <- as.double(all.history$H.Score)
all.history$A.Score <- as.double(all.history$A.Score)
## Run Elo
elo <- elo.run(score(H.Score, A.Score) ~ adjust(Home, 24) + Away + regress(Year, 1500, 0.5) + k(4*(abs(H.Score - A.Score)^(1/4))), data = all.history)
all.history$H.Elo <- elo[[1]][,7]
all.history$A.Elo <- elo[[1]][,8]
## Create Elo History Table
home <- all.history %>% select(Date, Home, H.Elo)
away <- all.history %>% select(Date, Away, A.Elo)
cols <- c("Date", "Team", "Elo")
colnames(home) <- cols
colnames(away) <- cols
elo.history <- rbind(home,away)
elo.history$Elo <- round(elo.history$Elo, digits = 0)
## Current Elos
current.elos <- elo.history %>%
group_by(Team) %>%
arrange(desc(Date)) %>%
slice(1) %>% select(-Date) %>%
arrange(desc(Elo))
current.elos$Rank <- 1:10
write_csv(current.elos, "currentelos.csv")
## Ten Team Era for History Page
tenteam <- elo.history %>% filter(Date > ISOdate(2013,1,1))
write_csv(tenteam,"elohistory.csv")
## Simulations
nsim <- 100000
all.schedule$Prediction <- predict(elo, newdata = all.schedule)
all.schedule <- data.frame(all.schedule, Round = rep(1:nsim, each = nrow(all.schedule)))
all.schedule$Sim <- runif((nrow(all.schedule)),0,1)
all.schedule <- mutate(all.schedule, H.Win = ifelse(Sim < Prediction, 1, 0))
all.schedule <- mutate(all.schedule, Winner = ifelse(H.Win == 1, as.character(Home), as.character(Away)))
standings <- all.schedule %>% group_by(Winner, Round) %>% count()
standings <- standings %>% left_join(wins, by = "Winner")
standings$n.y[is.na(standings$n.y)] <- 0
standings$n <- standings$n.x + standings$n.y
standings$tiebreak <- runif((nrow(standings)),0,1)
standings <- standings %>% arrange(tiebreak) %>% arrange(desc(n)) %>% arrange(Round)
standings$Rank <- rep(1:10, max(standings$Round))
pred.1 <- standings %>% filter(Rank == 1) %>% group_by(Winner) %>% count()
pred.1$Pct <- (pred.1$n/nsim)
colnames(pred.1) <- c("Winner", "n", "First")
pred.2 <- standings %>% filter(Rank == 2) %>% group_by(Winner) %>% count()
pred.2$Pct <- (pred.2$n/nsim)
colnames(pred.2) <- c("Winner", "n", "Second")
pred.3 <- standings %>% filter(Rank == 3) %>% group_by(Winner) %>% count()
pred.3$Pct <- (pred.3$n/nsim)
colnames(pred.3) <- c("Winner", "n", "Third")
pred.4 <- standings %>% filter(Rank == 4) %>% group_by(Winner) %>% count()
pred.4$Pct <- (pred.4$n/nsim)
colnames(pred.4) <- c("Winner", "n", "Fourth")
pred.5 <- standings %>% filter(Rank == 5) %>% group_by(Winner) %>% count()
pred.5$Pct <- (pred.5$n/nsim)
colnames(pred.5) <- c("Winner", "n", "Fifth")
pred.6 <- standings %>% filter(Rank > 5) %>% group_by(Winner) %>% count()
pred.6$Pct <- (pred.6$n/nsim)
colnames(pred.6) <- c("Winner", "n", "Out")
predictions <- ungroup(as.data.frame(standings$Winner[1:10]))
colnames(predictions) <- "Winner"
predictions <- pred.1 %>% select(-n) %>% right_join(predictions, by = "Winner")
predictions <- pred.2 %>% select(-n) %>% right_join(predictions, by = "Winner")
predictions <- pred.3 %>% select(-n) %>% right_join(predictions, by = "Winner")
predictions <- pred.4 %>% select(-n) %>% right_join(predictions, by = "Winner")
predictions <- pred.5 %>% select(-n) %>% right_join(predictions, by = "Winner")
predictions <- pred.6 %>% select(-n) %>% right_join(predictions, by = "Winner")
predictions$Team <- predictions$Winner
predictions <- predictions %>% left_join(current.elos, by = "Team")
predictions <- predictions %>% ungroup() %>% select(-Winner)
predictions <- predictions %>%
mutate(First = percent(First, 1)) %>%
mutate(Second = percent(Second, 1)) %>%
mutate(Third = percent(Third, 1)) %>%
mutate(Fourth = percent(Fourth, 1)) %>%
mutate(Fifth = percent(Fifth, 1)) %>%
mutate(Out = percent(Out, 1))
predictions <- predictions %>% select(Rank, Team, Elo, First, Second, Third, Fourth, Fifth, Out)
write_csv(predictions, "predictions.csv")
## Render Elo Pages
render("~/Google Drive/KBO/kbofancystats/content/elo.Rmd")
render("~/Google Drive/KBO/kbofancystats/content/elo2020.Rmd")
render("~/Google Drive/KBO/kbofancystats/content/elohistory.Rmd")
setwd("~/Google Drive/KBO/kbofancystats")
#stop_server()
serve_site()
## Homepage image
setwd("~/Google Drive/KBO/kbofancystats/static/img")
tenteam <- read_csv("~/Google Drive/KBO/elohistory.csv")
tenteam <- tenteam %>% subset(Date > ISOdate(2020,5,4))
tenteam$Team <- as.factor(tenteam$Team)
rank <- read_csv("~/Google Drive/KBO/currentelos.csv")
rank$Team2 <- paste0(rank$Team, " (",rank$Elo,")")
levels <- rank$Team2
tenteam <- tenteam %>% left_join(rank, by = "Team")
tenteam$Team2 <- as.factor(tenteam$Team2)
tenteam$Team2 <- factor(tenteam$Team2, levels = levels)
plot <- ggplot() + geom_line(data = tenteam, aes(Date, Elo.x, color = Team2)) + theme_minimal() + labs(x = "", y = "Elo", color = "")
ggsave("elo.png", width = 8, height = 3)
View(standings)
View(predictions)
predictions[is.na(predictions)]
predictions[is.na(predictions)] <- "0%"
View(predictions)
write_csv(predictions, "predictions.csv")
## Render Elo Pages
render("~/Google Drive/KBO/kbofancystats/content/elo.Rmd")
render("~/Google Drive/KBO/kbofancystats/content/elo2020.Rmd")
render("~/Google Drive/KBO/kbofancystats/content/elohistory.Rmd")
setwd("~/Google Drive/KBO/kbofancystats")
#stop_server()
serve_site()
## Homepage image
setwd("~/Google Drive/KBO/kbofancystats/static/img")
tenteam <- read_csv("~/Google Drive/KBO/elohistory.csv")
tenteam <- tenteam %>% subset(Date > ISOdate(2020,5,4))
tenteam$Team <- as.factor(tenteam$Team)
rank <- read_csv("~/Google Drive/KBO/currentelos.csv")
rank$Team2 <- paste0(rank$Team, " (",rank$Elo,")")
levels <- rank$Team2
tenteam <- tenteam %>% left_join(rank, by = "Team")
tenteam$Team2 <- as.factor(tenteam$Team2)
tenteam$Team2 <- factor(tenteam$Team2, levels = levels)
plot <- ggplot() + geom_line(data = tenteam, aes(Date, Elo.x, color = Team2)) + theme_minimal() + labs(x = "", y = "Elo", color = "")
ggsave("elo.png", width = 8, height = 3)
View(predictions)
setwd("~/Google Drive/KBO")
write_csv(predictions, "predictions.csv")
## Render Elo Pages
render("~/Google Drive/KBO/kbofancystats/content/elo.Rmd")
render("~/Google Drive/KBO/kbofancystats/content/elo2020.Rmd")
render("~/Google Drive/KBO/kbofancystats/content/elohistory.Rmd")
setwd("~/Google Drive/KBO/kbofancystats")
#stop_server()
serve_site()
## Homepage image
setwd("~/Google Drive/KBO/kbofancystats/static/img")
tenteam <- read_csv("~/Google Drive/KBO/elohistory.csv")
tenteam <- tenteam %>% subset(Date > ISOdate(2020,5,4))
tenteam$Team <- as.factor(tenteam$Team)
rank <- read_csv("~/Google Drive/KBO/currentelos.csv")
rank$Team2 <- paste0(rank$Team, " (",rank$Elo,")")
levels <- rank$Team2
tenteam <- tenteam %>% left_join(rank, by = "Team")
tenteam$Team2 <- as.factor(tenteam$Team2)
tenteam$Team2 <- factor(tenteam$Team2, levels = levels)
plot <- ggplot() + geom_line(data = tenteam, aes(Date, Elo.x, color = Team2)) + theme_minimal() + labs(x = "", y = "Elo", color = "")
ggsave("elo.png", width = 8, height = 3)
stop_server()
